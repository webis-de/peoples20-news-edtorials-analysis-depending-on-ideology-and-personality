{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elba_ro\\Documents\\projects\\nlpcss2020-editorials-analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elba_ro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elba_ro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "%cd ..\n",
    "import textmining.text_miner \n",
    "import textmining.loader  as loader\n",
    "import textmining.topic_modeler as tm\n",
    "import textmining.significance_testing as significance_testing\n",
    "import textmining.utility as utility\n",
    "import textmining.analysis as analysis\n",
    "import textmining.cluster_analysis as cluster_analysis\n",
    "import textmining.machine_learning as machine_learning\n",
    "import textmining.news_editorials_experiments as experiment\n",
    "import textmining.cross_models_significance as cross\n",
    "\n",
    "importlib.reload(textmining.machine_learning)\n",
    "importlib.reload(textmining.news_editorials_experiments)\n",
    "importlib.reload(textmining.cross_models_significance)\n",
    "importlib.reload(textmining.cluster_analysis)\n",
    "\n",
    "importlib.reload(textmining.topic_modeler)\n",
    "importlib.reload(textmining.analysis)\n",
    "importlib.reload(textmining.loader)\n",
    "os.chdir(current_dir)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait = 'agreeableness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"corpus\" is set. It contains the 6000 annotation\n"
     ]
    }
   ],
   "source": [
    "load = loader.loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "personality_df = load.load_personality_traits()\n",
    "personality_df.set_index('id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HIGH    14\n",
       "LOW     10\n",
       "Name: agreeableness, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traints\n",
    "def set_avg_to_low(row):\n",
    "    row[trait] = 'LOW' if row[trait] == 'AVERAGE' else row[trait]\n",
    "    return row\n",
    "personality_df = personality_df.apply(set_avg_to_low, axis=1)\n",
    "personality_df[[trait]]\n",
    "personality_df[trait].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"corpus\" is set with personality: \"a\" , \"b\", ...\n"
     ]
    }
   ],
   "source": [
    "corpus = load.corpus\n",
    "corpus = load.add_personality_label(personality_df[[trait]], trait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HIGH    3500\n",
       "LOW     2500\n",
       "Name: agreeableness, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[trait].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(effect_abstracted    1     2     3\n",
       " agreeableness                     \n",
       " HIGH               380  1050  2070\n",
       " LOW                307   940  1253,\n",
       " None,\n",
       " None,\n",
       " effect          1    2     3     4    5\n",
       " agreeableness                          \n",
       " HIGH           71  309  1050  1356  714\n",
       " LOW            72  235   940   844  409)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(corpus[trait], corpus.effect_abstracted), print(''),print(''), pd.crosstab(corpus[trait], corpus.effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles dataframe for ideology HIGH was created\n",
      "The id of the df is the article id without txt\n",
      "total con: 979\n",
      "rounded TRAINGING data:  783\n",
      "rounded Validation data:  0\n",
      "rounded Test data:  196\n",
      "total con: 1000\n",
      "rounded TRAINGING data:  800\n",
      "rounded Validation data:  0\n",
      "rounded Test data:  200\n",
      "\"data_division\" is set as dict with keys  dict_keys(['train', 'test'])\n",
      "\"corpus\" is set with split_label: \"train\" \"test\"\n",
      "\"data_division\" is set as dict with keys  dict_keys(['train', 'test'])\n",
      "length of self.df:  979\n",
      "length of self.data_division_df:  979\n",
      "length of self.df:  979\n",
      "articles dataframe for ideology LOW was created\n",
      "The id of the df is the article id without txt\n",
      "total con: 979\n",
      "rounded TRAINGING data:  783\n",
      "rounded Validation data:  0\n",
      "rounded Test data:  196\n",
      "total con: 1000\n",
      "rounded TRAINGING data:  800\n",
      "rounded Validation data:  0\n",
      "rounded Test data:  200\n",
      "\"data_division\" is set as dict with keys  dict_keys(['train', 'test'])\n",
      "\"corpus\" is set with split_label: \"train\" \"test\"\n",
      "\"data_division\" is set as dict with keys  dict_keys(['train', 'test'])\n",
      "length of self.df:  979\n",
      "length of self.data_division_df:  979\n",
      "length of self.df:  979\n"
     ]
    }
   ],
   "source": [
    "agreeable_maj_df = load.get_article_dfs_per_ideology(ideology = trait, include_content = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_df = agreeable_maj_df['HIGH']\n",
    "#average_df = agreeable_maj_df['AVERAGE']\n",
    "low_df = agreeable_maj_df['LOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split_label</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>majority</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>challenging</th>\n",
       "      <td>30</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_effect</th>\n",
       "      <td>76</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reinforcing</th>\n",
       "      <td>90</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split_label  test  train\n",
       "majority                \n",
       "challenging    30    123\n",
       "no_effect      76    291\n",
       "reinforcing    90    369"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(low_df.majority, low_df.split_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split_label</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>majority</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>challenging</th>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_effect</th>\n",
       "      <td>43</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reinforcing</th>\n",
       "      <td>136</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split_label  test  train\n",
       "majority                \n",
       "challenging    17     86\n",
       "no_effect      43    162\n",
       "reinforcing   136    535"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(high_df.majority, high_df.split_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_if_all_0(df):\n",
    "    return  df[(df['challenging']== 0) & (df['no_effect']== 0) & (df['reinforcing']== 0)]\n",
    "\n",
    "check_if_all_0(high_df)\n",
    "check_if_all_0(low_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinforcing    671\n",
      "no_effect      205\n",
      "challenging    103\n",
      "Name: majority, dtype: int64\n",
      "\n",
      "reinforcing    459\n",
      "no_effect      367\n",
      "challenging    153\n",
      "Name: majority, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(high_df.majority.value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "#print(average_df.majority.value_counts())\n",
    "#print()\n",
    "\n",
    "print(low_df.majority.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train    783\n",
      "test     196\n",
      "Name: split_label, dtype: int64\n",
      "\n",
      "train    783\n",
      "test     196\n",
      "Name: split_label, dtype: int64\n",
      "\n",
      "\n",
      "split_label  test  train\n",
      "majority                \n",
      "challenging    17     86\n",
      "no_effect      43    162\n",
      "reinforcing   136    535\n",
      "\n",
      "split_label  test  train\n",
      "majority                \n",
      "challenging    30    123\n",
      "no_effect      76    291\n",
      "reinforcing    90    369\n"
     ]
    }
   ],
   "source": [
    "print(high_df.split_label.value_counts())\n",
    "print()\n",
    "#print(average_df.split_label.value_counts())\n",
    "#print()\n",
    "print(low_df.split_label.value_counts())\n",
    "print()\n",
    "print()\n",
    "print(pd.crosstab(high_df.majority, high_df.split_label))\n",
    "print()\n",
    "#print(pd.crosstab(average_df.majority, average_df.split_label))\n",
    "#print()\n",
    "print(pd.crosstab(low_df.majority, low_df.split_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_df.rename(columns = {'majority': trait+'_high_majority'}, inplace = True)\n",
    "#average_df.rename(columns = {'majority': trait+'_average_majority'}, inplace = True)\n",
    "low_df.rename(columns = {'majority': trait+'_low_majority'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/articles_with_adu_liwc_lexicons_content.json', orient='records')\n",
    "data.set_index('idx', inplace=True)\n",
    "print(len(data))\n",
    "data = data.merge(high_df[[trait+'_high_majority']], how=\"inner\",left_index=True, right_index=True )\n",
    "#data = data.merge(average_df[[trait+'_average_majority']], how=\"inner\",left_index=True, right_index=True )\n",
    "data = data.merge(low_df[[trait+'_low_majority']], how=\"inner\",left_index=True, right_index=True )\n",
    "\n",
    "data.drop(['liberal_majority', 'conservative_majority'], axis=1, inplace=True)\n",
    "data.to_csv('../data/'+trait+'_HAL_all_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/'+trait+'_HAL_all_features.csv', index_col='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df, test_df = analysis.get_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiments for ideologies:  ['agreeableness_high_majority', 'agreeableness_low_majority'] \n",
      " remove_outliers:  True \n",
      " normalize:  sqrt\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n",
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "('liwc',)\n",
      "performing gridsearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-18 07:15:19,794 : ERROR : Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 909, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 562, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\concurrent\\futures\\_base.py\", line 427, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\threading.py\", line 296, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-28-1947f6312144>\", line 6, in <module>\n",
      "    normalize=\"sqrt\")\n",
      "  File \"C:\\Users\\elba_ro\\Documents\\projects\\nlpcss2020-editorials-analysis\\textmining\\news_editorials_experiments.py\", line 121, in run_experiments\n",
      "    result = validate_train_test(X_train_df, y_train, X_test_df, y_test, feature_types, validation_folds=5)\n",
      "  File \"C:\\Users\\elba_ro\\Documents\\projects\\nlpcss2020-editorials-analysis\\textmining\\news_editorials_experiments.py\", line 86, in validate_train_test\n",
      "    best_params = machine_learning.svc_param_gridsearch(X_train, y_train, nfolds_or_division=validation_folds)\n",
      "  File \"C:\\Users\\elba_ro\\Documents\\projects\\nlpcss2020-editorials-analysis\\textmining\\machine_learning.py\", line 167, in svc_param_gridsearch\n",
      "    grid_search.fit(X, y)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 710, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 1151, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 689, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1017, in __call__\n",
      "    self.retrieve()\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 931, in retrieve\n",
      "    backend.abort_everything(ensure_ready=ensure_ready)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 579, in abort_everything\n",
      "    self._workers.shutdown(kill_workers=True)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1101, in shutdown\n",
      "    qmt.join()\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"c:\\programdata\\miniconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3318\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3319\u001b[1;33m                     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3320\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-1947f6312144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                               \u001b[0mremove_outliers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                               normalize=\"sqrt\")\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\projects\\nlpcss2020-editorials-analysis\\textmining\\news_editorials_experiments.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[1;34m(df, ideologies, filename, remove_outliers, normalize)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\nlpcss2020-editorials-analysis\\textmining\\news_editorials_experiments.py\u001b[0m in \u001b[0;36mvalidate_train_test\u001b[1;34m(X_train_df, y_train, X_test_df, y_test, feature_types, validation_folds)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m#else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmachine_learning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvc_param_gridsearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfolds_or_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\nlpcss2020-editorials-analysis\\textmining\\machine_learning.py\u001b[0m in \u001b[0;36msvc_param_gridsearch\u001b[1;34m(X, y, nfolds_or_division)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    930\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[1;34m(self, ensure_ready)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \"\"\"\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[1;34m(self, wait, kill_workers)\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m                 \u001b[0mqmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2033\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2034\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2035\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3335\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3336\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3337\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3338\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2035\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2037\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1418\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1318\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m             )\n\u001b[0;32m   1320\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[0mformatted_exceptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "\n",
    "results_sqrt = experiment.run_experiments(df, ideologies=[trait+'_high_majority', \n",
    "                                                              #trait+'_average_majority',\n",
    "                                                             trait+'_low_majority',],\n",
    "                                              filename=\"../out/style_content_models_results/\"+trait+\"_HAL_experiments_sqrt\",\n",
    "                                              remove_outliers=True,\n",
    "                                              normalize=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment.train_baseline(df, trait+'_high_majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment.train_baseline(df, trait+'_average_majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment.train_baseline(df, trait+'_low_majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in ['low', 'high']:\n",
    "    print(l)\n",
    "    print(results_minmax[ '{}_{}_majority'.format(trait, l)].macro.max() )\n",
    "    print(results_log[ '{}_{}_majority'.format(trait, l)].macro.max() )\n",
    "    print(results_sqrt[ '{}_{}_majority'.format(trait, l)].macro.max() )\n",
    "    print(results_standard[ '{}_{}_majority'.format(trait, l)].macro.max() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n",
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('lemma',)) 2\n",
      "('dummy',)\n",
      "('lemma',)\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('liwc', 'nrc', 'mpqa_arg')) 2\n",
      "('dummy',)\n",
      "('liwc', 'nrc', 'mpqa_arg')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('nrc', 'adu')) 2\n",
      "('dummy',)\n",
      "('nrc', 'adu')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')) 2\n",
      "('dummy',)\n",
      "('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('lemma',), ('liwc', 'nrc', 'mpqa_arg')) 2\n",
      "('lemma',)\n",
      "('liwc', 'nrc', 'mpqa_arg')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('lemma',), ('nrc', 'adu')) 2\n",
      "('lemma',)\n",
      "('nrc', 'adu')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('lemma',), ('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')) 2\n",
      "('lemma',)\n",
      "('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('liwc', 'nrc', 'mpqa_arg'), ('nrc', 'adu')) 2\n",
      "('liwc', 'nrc', 'mpqa_arg')\n",
      "('nrc', 'adu')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('liwc', 'nrc', 'mpqa_arg'), ('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')) 2\n",
      "('liwc', 'nrc', 'mpqa_arg')\n",
      "('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_high_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('nrc', 'adu'), ('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')) 2\n",
      "('nrc', 'adu')\n",
      "('mpqa_arg', 'mpqa_subjobg', 'adu', 'lemma')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "level= 'high'\n",
    "PATH_RESULT= '../out/style_content_models_results/{}_HAL_experiments_{}_{}_{}_majority.csv'\n",
    "\n",
    "high_cross_models_sqrt_df    = cross.run_model_pairs_significance(PATH_RESULT.format(trait,'sqrt',trait, level), \n",
    "                                                                  df, '{}_{}_majority'.format(trait, level), \n",
    "                                                                  'sqrt'    , 'sqrt_{}_{}'.format(trait, level) \n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_pair</th>\n",
       "      <th>is_normal</th>\n",
       "      <th>stat</th>\n",
       "      <th>p_val</th>\n",
       "      <th>wilk_stat</th>\n",
       "      <th>wilk_p_val</th>\n",
       "      <th>significant</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((dummy,), (lemma,))</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.232330</td>\n",
       "      <td>0.827684</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.685830</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((dummy,), (liwc, nrc, mpqa_arg))</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.442074</td>\n",
       "      <td>0.681286</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500184</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((dummy,), (nrc, adu))</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.183643</td>\n",
       "      <td>0.863227</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.685830</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((dummy,), (mpqa_arg, mpqa_subjobg, adu, lemma))</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.517539</td>\n",
       "      <td>0.203735</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.138011</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((lemma,), (liwc, nrc, mpqa_arg))</td>\n",
       "      <td>True</td>\n",
       "      <td>0.094551</td>\n",
       "      <td>0.929218</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.892738</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('lemma',)': [0.17448405253283303, 0.2810957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((lemma,), (nrc, adu))</td>\n",
       "      <td>True</td>\n",
       "      <td>0.094589</td>\n",
       "      <td>0.929190</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.892738</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('lemma',)': [0.17448405253283303, 0.2810957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((lemma,), (mpqa_arg, mpqa_subjobg, adu, lemma))</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.097920</td>\n",
       "      <td>0.333890</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.345231</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('lemma',)': [0.17448405253283303, 0.2810957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((liwc, nrc, mpqa_arg), (nrc, adu))</td>\n",
       "      <td>True</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.981272</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.892738</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('liwc', 'nrc', 'mpqa_arg')': [0.28707893413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((liwc, nrc, mpqa_arg), (mpqa_arg, mpqa_subjob...</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.072010</td>\n",
       "      <td>0.344087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.345231</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('liwc', 'nrc', 'mpqa_arg')': [0.28707893413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((nrc, adu), (mpqa_arg, mpqa_subjobg, adu, lem...</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.130589</td>\n",
       "      <td>0.321425</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.345231</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('nrc', 'adu')': [0.28819875776397513, 0.372...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_pair  is_normal      stat  \\\n",
       "0                               ((dummy,), (lemma,))       True -0.232330   \n",
       "1                  ((dummy,), (liwc, nrc, mpqa_arg))      False -0.442074   \n",
       "2                             ((dummy,), (nrc, adu))       True -0.183643   \n",
       "3   ((dummy,), (mpqa_arg, mpqa_subjobg, adu, lemma))       True -1.517539   \n",
       "4                  ((lemma,), (liwc, nrc, mpqa_arg))       True  0.094551   \n",
       "5                             ((lemma,), (nrc, adu))       True  0.094589   \n",
       "6   ((lemma,), (mpqa_arg, mpqa_subjobg, adu, lemma))       True -1.097920   \n",
       "7                ((liwc, nrc, mpqa_arg), (nrc, adu))       True  0.024974   \n",
       "8  ((liwc, nrc, mpqa_arg), (mpqa_arg, mpqa_subjob...       True -1.072010   \n",
       "9  ((nrc, adu), (mpqa_arg, mpqa_subjobg, adu, lem...       True -1.130589   \n",
       "\n",
       "      p_val  wilk_stat  wilk_p_val  significant  \\\n",
       "0  0.827684        6.0    0.685830        False   \n",
       "1  0.681286        5.0    0.500184        False   \n",
       "2  0.863227        6.0    0.685830        False   \n",
       "3  0.203735        2.0    0.138011        False   \n",
       "4  0.929218        7.0    0.892738        False   \n",
       "5  0.929190        7.0    0.892738        False   \n",
       "6  0.333890        4.0    0.345231        False   \n",
       "7  0.981272        7.0    0.892738        False   \n",
       "8  0.344087        4.0    0.345231        False   \n",
       "9  0.321425        4.0    0.345231        False   \n",
       "\n",
       "                                                data  \n",
       "0  {'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...  \n",
       "1  {'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...  \n",
       "2  {'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...  \n",
       "3  {'('dummy',)': [0.27, 0.31, 0.36, 0.27, 0.35],...  \n",
       "4  {'('lemma',)': [0.17448405253283303, 0.2810957...  \n",
       "5  {'('lemma',)': [0.17448405253283303, 0.2810957...  \n",
       "6  {'('lemma',)': [0.17448405253283303, 0.2810957...  \n",
       "7  {'('liwc', 'nrc', 'mpqa_arg')': [0.28707893413...  \n",
       "8  {'('liwc', 'nrc', 'mpqa_arg')': [0.28707893413...  \n",
       "9  {'('nrc', 'adu')': [0.28819875776397513, 0.372...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_cross_models_sqrt_df[high_cross_models_sqrt_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n",
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_low_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('lemma',)) 2\n",
      "('dummy',)\n",
      "('lemma',)\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_low_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('liwc', 'adu')) 2\n",
      "('dummy',)\n",
      "('liwc', 'adu')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_low_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('dummy',), ('liwc', 'adu', 'lemma')) 2\n",
      "('dummy',)\n",
      "('liwc', 'adu', 'lemma')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_low_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('lemma',), ('liwc', 'adu')) 2\n",
      "('lemma',)\n",
      "('liwc', 'adu')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_low_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('lemma',), ('liwc', 'adu', 'lemma')) 2\n",
      "('lemma',)\n",
      "('liwc', 'adu', 'lemma')\n",
      "preprocessing data...\n",
      "removing outliers by clipping values...\n",
      "getting only numeric features from the training set...\n",
      "There are 428  numeric features out of 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting X y data...\n",
      "Normalizing by using sqrt scaler...\n",
      "normalized\n",
      "end of get_x_y.\n",
      "END of preprocessing\n",
      "agreeableness_low_majority\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n",
      "(('liwc', 'adu'), ('liwc', 'adu', 'lemma')) 2\n",
      "('liwc', 'adu')\n",
      "('liwc', 'adu', 'lemma')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "level= 'low'\n",
    "PATH_RESULT= '../out/style_content_models_results/{}_HAL_experiments_{}_{}_{}_majority.csv'\n",
    "\n",
    "low_cross_models_sqrt_df    = cross.run_model_pairs_significance(PATH_RESULT.format(trait,'sqrt',trait, level), \n",
    "                                                                  df, '{}_{}_majority'.format(trait, level), \n",
    "                                                                  'sqrt'    , 'sqrt_{}_{}'.format(trait, level) \n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_pair</th>\n",
       "      <th>is_normal</th>\n",
       "      <th>stat</th>\n",
       "      <th>p_val</th>\n",
       "      <th>wilk_stat</th>\n",
       "      <th>wilk_p_val</th>\n",
       "      <th>significant</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((dummy,), (lemma,))</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.107782</td>\n",
       "      <td>0.330081</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.345231</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.42, 0.29, 0.29, 0.35, 0.14],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((dummy,), (liwc, adu))</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.914850</td>\n",
       "      <td>0.128028</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.138011</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.42, 0.29, 0.29, 0.35, 0.14],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((dummy,), (liwc, adu, lemma))</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.582713</td>\n",
       "      <td>0.061152</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079616</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('dummy',)': [0.42, 0.29, 0.29, 0.35, 0.14],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((lemma,), (liwc, adu))</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.680818</td>\n",
       "      <td>0.168093</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.138011</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('lemma',)': [0.34722222222222215, 0.3429211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((lemma,), (liwc, adu, lemma))</td>\n",
       "      <td>True</td>\n",
       "      <td>-4.604875</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>True</td>\n",
       "      <td>{'('lemma',)': [0.34722222222222215, 0.3429211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((liwc, adu), (liwc, adu, lemma))</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.947811</td>\n",
       "      <td>0.396896</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.345231</td>\n",
       "      <td>False</td>\n",
       "      <td>{'('liwc', 'adu')': [0.3753258390355165, 0.474...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model_pair  is_normal      stat     p_val  \\\n",
       "0               ((dummy,), (lemma,))       True -1.107782  0.330081   \n",
       "1            ((dummy,), (liwc, adu))       True -1.914850  0.128028   \n",
       "2     ((dummy,), (liwc, adu, lemma))       True -2.582713  0.061152   \n",
       "3            ((lemma,), (liwc, adu))       True -1.680818  0.168093   \n",
       "4     ((lemma,), (liwc, adu, lemma))       True -4.604875  0.009994   \n",
       "5  ((liwc, adu), (liwc, adu, lemma))       True -0.947811  0.396896   \n",
       "\n",
       "   wilk_stat  wilk_p_val  significant  \\\n",
       "0        4.0    0.345231        False   \n",
       "1        2.0    0.138011        False   \n",
       "2        1.0    0.079616        False   \n",
       "3        2.0    0.138011        False   \n",
       "4        0.0    0.043114         True   \n",
       "5        4.0    0.345231        False   \n",
       "\n",
       "                                                data  \n",
       "0  {'('dummy',)': [0.42, 0.29, 0.29, 0.35, 0.14],...  \n",
       "1  {'('dummy',)': [0.42, 0.29, 0.29, 0.35, 0.14],...  \n",
       "2  {'('dummy',)': [0.42, 0.29, 0.29, 0.35, 0.14],...  \n",
       "3  {'('lemma',)': [0.34722222222222215, 0.3429211...  \n",
       "4  {'('lemma',)': [0.34722222222222215, 0.3429211...  \n",
       "5  {'('liwc', 'adu')': [0.3753258390355165, 0.474...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_cross_models_sqrt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
